# Hyper_parameter_ Tuning_2019-12-02

ν”Όμ³λ¥Ό λ‹¤ μ •ν•΄μ£Όκ³  λ¨λ“  μ‘μ—…μ„ λ§μΉ ν›„μ— μ΄ λ¨λΈμ μ„±λ¥μ„ μµμ ν™”ν•κ³  μ‹¶μ„ λ• ν•λ” μ‘μ—… 

κ²°κ³Όκ°’ μ°¨μ΄λ” +- 5 % 

    y = aX

a λ” λ¨Έμ‹ μ΄ ν•™μµν•΄μ„ μ •ν•΄μ£Όλ” κ°’

`e.g.` Decision Tree 

criterion = 'entropy', 'gini'

μλ„ 100 μΌ λ•κΉμ§€ ν•™μµν•λ©΄ overfitting

max_depth, max_leafnode λ¥Ό μ •ν•΄μ£Όλ” κ²ƒ

# Hyperparameter Tuning λ°©λ²•

1. Manual Search μλ™μΌλ΅ ν•λ‚μ”© λ°”κΏ”μ£Όλ”κ²ƒ

2. Grid Search : ν•μ΄νΌνλΌλ―Έν„° λ²”μ„λ¥Ό μ •ν•΄μ¤€λ‹¤(λ²”μ„λ” Manual Search λ΅ μ •ν•΄μ¤€λ‹¤)

    searching νμλ” νλΌλ―Έν„° ν•λ‚ * νλΌλ―Έν„° ν•λ‚ * cv(cross validation) μ

    `e.g.` max_depth λ¥Ό 10 λ¶€ν„° 20 κΉμ§€ 1 λ‹¨μ„λ΅ λ³Έλ‹¤

    `e.g.` κ²°μ •ν•  νλΌλ―Έν„°κ°€ λ§λ‹¤λ©΄ pair λ΅ λ„£μ–΄μ¤€λ‹¤

    π‘ μ„¤μ •ν•΄μ¤€ κ°’λ§ μ‹λ„ν•΄λ³Έλ‹¤

3. Random Search

    λ²”μ„λ¥Ό μ£Όλ©΄ Uniform distribution μΌλ΅ λλ¤ν•κ² μ‹λ„

    n_iter λ½‘λ” νμ

    search λ²”μ„κ°€ λμ–΄λ‚λ©΄ n_iter λ„ λ‘κ°™μ΄ λλ ¤μ¤€λ‹¤

    μ–΄λ””κ°€ μµμ μ μΌμ§€ μμƒκ°€λ¥ν•λ‹¤λ©΄ Distributionμ„ Gaussian Distribution λ“±μΌλ΅ λ°”κΎΌλ‹¤. 

    `e.g.` n_iter = 10 

    π‘ λ‹¤μ–‘ν• μ«μλ“¤μ„ μ‹λ„ν•κΈ° λ•λ¬Έμ— μµμ μ μ΄ λ‚μ¬ ν™•λ¥ μ΄ λ†’λ‹¤

    π‘ μµμ μ  κ·Όμ²λ΅ μ„μΉ λ²”μ„λ¥Ό μΆν€μ„ λ‹¤μ‹ μ‹λ„ν•  μ μλ‹¤

Machine Learning μ—μ„ Learning rate κ°€ μ¤‘μ”ν•λ‹¤

Random Forest μ—μ„ n_estimator : Decision Tree μ κ°―μ

(!)Random ν• μ„±κ²©μ Model μ„ μ“Έ λ•λ” λ¬΄μ΅°κ±΄ Random_state λ¥Ό μ„¤μ •ν•΄μ„ λ³µμ›κ°€λ¥ν•κ² ν•λ‹¤(!)

**n_estimator λ” ν΄μλ΅ μΆ‹μΌλ‹κΉ**

1. best parameter μ°Ύμ•„μ„
2. dict λ΅ μ €μ¥ν•΄μ„
3. n_estimator λ§ ν° μ«μλ΅ λ°”κΏ”μ„ model μƒλ΅ λ§λ“¤κ³  fit ν•λ‹¤